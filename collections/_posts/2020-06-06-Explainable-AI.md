---
title: Explainable AI
author: Michael Chary
layout: post
date: 2020-06-06
modified: 2020-06-10
tag: [explainable artificial intelligence] 
excerpt: domain specific interpretation
---



A crticism of artificial intelligence is that it is a _black box_, meaning that the process by which it transforms inputs to outputs is opaque. This opacity is not acceptable in healthcare where accountability requires transparency and, sometimes, the process is as important as the output. 

{% cite thiagarajan2020calibrating --file not-my-works %} published a [preprint](https://arxiv.org/pdf/2004.14480.pdf) where they provide greater insight into what features a neural network uses to classify dermatologic lesions. Their approach is to identify a latent space with statistically independent dimensions.



### Interval Calibration 


### Latent Spaces 
  Longer discussion [here](./2020-06-11-analysis-of-latent-spaces.md)


- How to do with language
- Counterfactual reasoning
- Similarity to Eve Marder's work
- Could also look at latent spaces across receptors

### Bibliography
{% bibliography --file not-my-works --cited %}